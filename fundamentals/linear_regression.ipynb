{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "792e5b77",
   "metadata": {},
   "source": [
    "# Linear Regression with Python\n",
    "In this notebook you will learn how to implement linear regression using Python's popular libraries such as NumPy, Pandas, and Scikit-learn. Linear regression is a fundamental algorithm in machine learning used for predicting a continuous target variable based on one or more predictor variables.\n",
    "\n",
    "### Step 1: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0c7605",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74733b3a",
   "metadata": {},
   "source": [
    "### Step 2: Load the Dataset\n",
    "You can use any dataset suitable for regression tasks. For this example, we will use a toy 1D dataset and a more complex dataset from Scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc3d1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "B0 = -2\n",
    "B1 = 1.7\n",
    "N = 15\n",
    "X_min, X_max = 0, 5\n",
    "epsilon = 5e-1\n",
    "\n",
    "def true_func(X):\n",
    "    return B0 + B1 * X\n",
    "\n",
    "X_obs = np.random.rand(N) * (X_max-X_min) + X_min\n",
    "y_obs = true_func(X=X_obs) + np.random.randn(N)*epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab9b4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_obs, y_obs, color='blue', label='Observed data', s=50)\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Observed Data')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0afdb7",
   "metadata": {},
   "source": [
    "### Step 3: Exercise - Implement Linear Regression using Least Squares\n",
    "\n",
    "In this exercise, you will implement a function that computes the optimal parameters (β₀ and β₁) for linear regression using the **least squares method**.\n",
    "\n",
    "#### The Least Squares Method\n",
    "\n",
    "The least squares method minimizes the sum of squared residuals. For a linear model:\n",
    "\n",
    "$$y = \\beta_0 + \\beta_1 x$$\n",
    "\n",
    "The optimal parameters are found by solving the **normal equations**:\n",
    "\n",
    "$$\\begin{bmatrix} N & \\sum x_i \\\\ \\sum x_i & \\sum x_i^2 \\end{bmatrix} \\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\end{bmatrix} = \\begin{bmatrix} \\sum y_i \\\\ \\sum x_i y_i \\end{bmatrix}$$\n",
    "\n",
    "This can also be expressed as:\n",
    "\n",
    "$$\\beta = (X^T X)^{-1} X^T y$$\n",
    "\n",
    "where X is the design matrix with a column of ones and the feature values, and y is the target vector.\n",
    "\n",
    "#### Your Task\n",
    "\n",
    "Implement a function `fit_linear_regression(X, y)` that:\n",
    "- Takes observed data X and y as input\n",
    "- Returns the estimated parameters (β₀, β₁) using the least squares method\n",
    "- You can use NumPy's linear algebra functions (e.g., `np.linalg.solve()` or `np.linalg.inv()`) to solve the system of equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c82c3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement the fit_linear_regression function using least squares method\n",
    "def fit_linear_regression(X, y):\n",
    "    \"\"\"\n",
    "    Fit a linear regression model using the least squares method.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : array-like, shape (n_samples,)\n",
    "        Input feature values\n",
    "    y : array-like, shape (n_samples,)\n",
    "        Target values\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    beta0, beta1 : float\n",
    "        Intercept and slope parameters of the fitted linear model\n",
    "    \"\"\"\n",
    "    # Your implementation here\n",
    "    pass\n",
    "\n",
    "# Test your implementation with the observed data\n",
    "beta0_est, beta1_est = fit_linear_regression(X_obs, y_obs)\n",
    "print(f\"Estimated parameters: β₀ = {beta0_est:.4f}, β₁ = {beta1_est:.4f}\")\n",
    "print(f\"True parameters: β₀ = {B0}, β₁ = {B1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c69484",
   "metadata": {},
   "source": [
    "### Step 4: Exercise - Implement Linear Regression using Gradient Descent\n",
    "\n",
    "In this exercise, you will implement linear regression using the **gradient descent optimization algorithm**. This is an iterative method that adjusts the parameters step-by-step to minimize the loss function.\n",
    "\n",
    "#### The Gradient Descent Method\n",
    "\n",
    "The general form of the gradient descent update rule is:\n",
    "\n",
    "$$\\theta_t = \\theta_{t-1} - \\alpha \\nabla L(\\theta_{t-1})$$\n",
    "\n",
    "where:\n",
    "- $\\theta_t$ represents the parameters at iteration $t$\n",
    "- $\\alpha$ is the **learning rate** (also called step size), a hyperparameter that controls how large each update step is\n",
    "- $\\nabla L(\\theta)$ is the gradient of the loss function with respect to the parameters\n",
    "\n",
    "#### Learning Rate (α)\n",
    "\n",
    "The learning rate is a crucial hyperparameter:\n",
    "- **Too small**: Convergence is slow, requiring many iterations\n",
    "- **Too large**: The algorithm may overshoot the optimum and diverge\n",
    "- **Just right**: The algorithm converges efficiently to the optimum\n",
    "\n",
    "#### Mean Squared Error (MSE) Loss Function\n",
    "\n",
    "For linear regression, we minimize the Mean Squared Error:\n",
    "\n",
    "$$L(\\beta_0, \\beta_1) = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - (\\beta_0 + \\beta_1 x_i))^2$$\n",
    "\n",
    "#### Gradient of MSE\n",
    "\n",
    "The gradients with respect to each parameter are:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\beta_0} = -\\frac{2}{N} \\sum_{i=1}^{N} (y_i - (\\beta_0 + \\beta_1 x_i))$$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\beta_1} = -\\frac{2}{N} \\sum_{i=1}^{N} (y_i - (\\beta_0 + \\beta_1 x_i)) \\cdot x_i$$\n",
    "\n",
    "#### Your Task\n",
    "\n",
    "Implement a function `fit_linear_regression_gd(X, y, learning_rate, n_steps)` that:\n",
    "- Takes observed data X and y as input\n",
    "- Takes the learning rate (α) as a hyperparameter\n",
    "- Takes the number of iterations (n_steps) as a hyperparameter\n",
    "- Initializes β₀ and β₁ (you can start with 0)\n",
    "- Iteratively updates the parameters using the gradient descent rule\n",
    "- Returns the final estimated parameters (β₀, β₁)\n",
    "- Optionally returns the loss history for each iteration to visualize the convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9478bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement the fit_linear_regression_gd function using gradient descent\n",
    "def fit_linear_regression_gd(X, y, learning_rate=0.01, n_steps=1000):\n",
    "    \"\"\"\n",
    "    Fit a linear regression model using gradient descent optimization.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : array-like, shape (n_samples,)\n",
    "        Input feature values\n",
    "    y : array-like, shape (n_samples,)\n",
    "        Target values\n",
    "    learning_rate : float, default=0.01\n",
    "        The learning rate (alpha) controlling the step size of each update\n",
    "    n_steps : int, default=1000\n",
    "        The number of iterations to run\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    beta0, beta1 : float\n",
    "        Intercept and slope parameters of the fitted linear model\n",
    "    loss_history : list\n",
    "        Optional: The MSE loss at each iteration (for visualization)\n",
    "    \"\"\"\n",
    "    # Your implementation here\n",
    "    pass\n",
    "\n",
    "# Test your implementation with the observed data\n",
    "# Try different learning rates and number of steps\n",
    "beta0_gd, beta1_gd = fit_linear_regression_gd(X_obs, y_obs, learning_rate=0.01, n_steps=1000)\n",
    "print(f\"Gradient Descent: β₀ = {beta0_gd:.4f}, β₁ = {beta1_gd:.4f}\")\n",
    "print(f\"True parameters: β₀ = {B0}, β₁ = {B1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bce209",
   "metadata": {},
   "source": [
    "Extra: Experiment with different learning rates and number of iterations to see how they affect convergence and the final parameters. Plot the loss history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5233fb3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1039b42d",
   "metadata": {},
   "source": [
    "### Step 5: Linear Regression on the California Housing Dataset\n",
    "\n",
    "Now we will apply both linear regression methods to a real-world dataset: the California Housing dataset. This dataset contains information about houses in California with various features and their prices.\n",
    "\n",
    "The dataset includes 20,640 samples with 8 features each:\n",
    "- **MedInc**: Median income in block group\n",
    "- **HouseAge**: Median house age in block group\n",
    "- **AveRooms**: Average number of rooms per household\n",
    "- **AveBedrms**: Average number of bedrooms per household\n",
    "- **Population**: Block group population\n",
    "- **AveOccup**: Average number of occupants per household\n",
    "- **Latitude**: Block group latitude\n",
    "- **Longitude**: Block group longitude\n",
    "- **MedHouseVal**: Median house value in hundreds of thousands of dollars in block group (target variable)\n",
    "\n",
    "We will focus on predicting house prices (MedHouseVal) using single or multiple features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8d59bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the California Housing dataset\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "california = fetch_california_housing()\n",
    "X_full = pd.DataFrame(california.data, columns=california.feature_names)\n",
    "y = california.target\n",
    "\n",
    "print(\"Dataset shape:\", X_full.shape)\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(X_full.head())\n",
    "print(\"\\nDataset statistics:\")\n",
    "print(X_full.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15fd24a",
   "metadata": {},
   "source": [
    "#### Data Visualization\n",
    "\n",
    "Let's visualize the relationship between some key features and the house prices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695ca70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the relationship between selected features and house prices\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: MedInc (median income) vs MedHouseVal (price)\n",
    "axes[0, 0].scatter(X_full['MedInc'], y, alpha=0.5, color='blue')\n",
    "axes[0, 0].set_xlabel('Median Income (MedInc)')\n",
    "axes[0, 0].set_ylabel('Median House Price (MedHouseVal)')\n",
    "axes[0, 0].set_title('Median Income vs House Price')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: HouseAge vs MedHouseVal (price)\n",
    "axes[0, 1].scatter(X_full['HouseAge'], y, alpha=0.5, color='green')\n",
    "axes[0, 1].set_xlabel('Median House Age (HouseAge)')\n",
    "axes[0, 1].set_ylabel('Median House Price (MedHouseVal)')\n",
    "axes[0, 1].set_title('House Age vs House Price')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: AveRooms vs MedHouseVal (price)\n",
    "axes[1, 0].scatter(X_full['AveRooms'], y, alpha=0.5, color='red')\n",
    "axes[1, 0].set_xlabel('Average Number of Rooms (AveRooms)')\n",
    "axes[1, 0].set_ylabel('Median House Price (MedHouseVal)')\n",
    "axes[1, 0].set_title('Average Rooms vs House Price')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Distribution of house prices\n",
    "axes[1, 1].hist(y, bins=30, color='purple', alpha=0.7)\n",
    "axes[1, 1].set_xlabel('Median House Price (MedHouseVal)')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].set_title('Distribution of House Prices')\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4e08d1",
   "metadata": {},
   "source": [
    "#### Exercise 5.1: Linear Regression using Least Squares on California Housing (Multi-dimensional)\n",
    "\n",
    "Using the California Housing dataset, implement a **general least squares method** that works with an arbitrary number of input features.\n",
    "\n",
    "**Task:**\n",
    "1. Select multiple features from the California Housing dataset (e.g., 'MedInc', 'HouseAge', 'AveRooms')\n",
    "2. Generalize your `fit_linear_regression()` function to handle X with shape (n_samples, n_features), returning a coefficient vector of shape (n_features + 1,) including the intercept\n",
    "3. Use the matrix formulation: β = (X^T X)^-1 X^T y where X includes a column of ones for the intercept\n",
    "4. Compare the estimated parameters with scikit-learn's LinearRegression\n",
    "5. Calculate the Mean Squared Error (MSE) and R² score on the training data\n",
    "6. Analyze which features have the largest impact on house prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f778daa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select multiple features to predict house prices\n",
    "features = ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms']  # You can add or remove features\n",
    "X_california = X_full[features].values\n",
    "y_california = y\n",
    "\n",
    "print(f\"Input shape: {X_california.shape}\")\n",
    "print(f\"Target shape: {y_california.shape}\")\n",
    "print(f\"Number of features: {X_california.shape[1]}\")\n",
    "\n",
    "# Placeholder: general least-squares function signature and docstring\n",
    "def fit_linear_regression_multi(X, y):\n",
    "    \"\"\"\n",
    "    Fit linear regression using the closed-form least squares solution for multi-dimensional input.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Design matrix containing the input features (without the column of ones).\n",
    "    y : array-like, shape (n_samples,)\n",
    "        Target values.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    beta : ndarray, shape (n_features + 1,)\n",
    "        Coefficient vector where beta[0] is the intercept and the remaining entries\n",
    "        correspond to the features in the same order as the columns of X.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - The implementation should prepend a column of ones to X for the intercept and\n",
    "      compute beta = (X^T X)^{-1} X^T y (or use a numerically stable solver).\n",
    "    - You may use `np.linalg.solve` instead of explicitly inverting the matrix.\n",
    "    \"\"\"\n",
    "    # Student implementation goes here\n",
    "    raise NotImplementedError\n",
    "\n",
    "# Then compare with scikit-learn and calculate MSE and R²\n",
    "# Tip: Use these formulas for evaluation metrics:\n",
    "# MSE = mean((y_true - y_pred)^2)\n",
    "# R² = 1 - (SS_res / SS_tot) where SS_res = sum((y_true - y_pred)^2) and SS_tot = sum((y_true - mean(y_true))^2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a767ec",
   "metadata": {},
   "source": [
    "#### Exercise 5.2: Linear Regression using Gradient Descent on California Housing (Multi-dimensional)\n",
    "\n",
    "Now implement a **general gradient descent method** that works with an arbitrary number of input features. Use the same features as Exercise 5.1.\n",
    "\n",
    "**Task:**\n",
    "1. Generalize your `fit_linear_regression_gd()` function to handle X with shape (n_samples, n_features), returning a coefficient vector of shape (n_features + 1,) including the intercept\n",
    "2. For each parameter β_j (j = 0, 1, ..., n_features), compute the gradient:\n",
    "$$\\frac{\\partial L}{\\partial \\beta_j} = -\\frac{2}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i) \\cdot x_{ij}$$\n",
    "where $x_{i0} = 1$ for the intercept term\n",
    "3. Update all parameters simultaneously: β_j := β_j - α ∂L/∂β_j\n",
    "\n",
    "4. Experiment with different learning rates and number of steps7. Calculate and compare MSE and R² scores\n",
    "\n",
    "5. Visualize the loss curve to observe convergence behavior6. Compare results with the least squares method from Exercise 5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2294a769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder: general gradient-descent function signature and docstring\n",
    "def fit_linear_regression_gd_multi(X, y, learning_rate=0.01, n_steps=1000):\n",
    "    \"\"\"\n",
    "    Fit linear regression using gradient descent for multi-dimensional input.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Design matrix containing the input features (without the column of ones).\n",
    "    y : array-like, shape (n_samples,)\n",
    "        Target values.\n",
    "    learning_rate : float, default=0.01\n",
    "        Step size (alpha) used in the gradient descent updates.\n",
    "    n_steps : int, default=1000\n",
    "        Number of gradient descent iterations.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    beta : ndarray, shape (n_features + 1,)\n",
    "        Coefficient vector where beta[0] is the intercept and the remaining entries\n",
    "        correspond to the features in the same order as the columns of X.\n",
    "    loss_history : list\n",
    "        List with the MSE loss value at each iteration (length n_steps).\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - The implementation should prepend a column of ones to X for the intercept.\n",
    "    - For each iteration compute predictions y_pred = X @ beta and the gradient\n",
    "      for all parameters at once, then update beta = beta - learning_rate * gradient.\n",
    "    - The gradient for parameter j is: dL/d_beta_j = -(2/N) * sum_i (y_i - y_pred_i) * x_ij,\n",
    "      where x_i0 = 1 for the intercept.\n",
    "    \"\"\"\n",
    "    # Student implementation goes here\n",
    "    raise NotImplementedError\n",
    "\n",
    "# Example usage structure (uncomment and complete):\n",
    "# beta_gd, loss_history = fit_linear_regression_gd_multi(X_california, y_california, learning_rate=0.01, n_steps=1000)\n",
    "# print(f\"Gradient Descent coefficients: {beta_gd}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
